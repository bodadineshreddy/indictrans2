{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO2Tceaa3gcK7iSgJvTiQLI",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bodadineshreddy/indictrans2/blob/main/test.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3wfC-vJvn8xV"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "from datasets import load_dataset\n",
        "\n",
        "# Load dataset with streaming enabled (Base language: English)\n",
        "dataset = load_dataset(\"facebook/nllb\", \"eng_Latn\", streaming=True)\n",
        "\n",
        "# Function to take only N samples\n",
        "def take_n_samples(iterable, n):\n",
        "    return (x for i, x in enumerate(iterable) if i < n)\n",
        "\n",
        "# Extract 10,000 samples for English → Telugu\n",
        "en_to_te = take_n_samples(\n",
        "    ({\"src\": x[\"translation\"][\"eng_Latn\"], \"tgt\": x[\"translation\"][\"tel_Telu\"]}\n",
        "     for x in dataset[\"train\"] if \"tel_Telu\" in x[\"translation\"]),\n",
        "    10_000\n",
        ")\n",
        "\n",
        "# Extract 10,000 samples for Telugu → English\n",
        "te_to_en = take_n_samples(\n",
        "    ({\"src\": x[\"translation\"][\"tel_Telu\"], \"tgt\": x[\"translation\"][\"eng_Latn\"]}\n",
        "     for x in dataset[\"train\"] if \"tel_Telu\" in x[\"translation\"]),\n",
        "    10_000\n",
        ")\n",
        "\n",
        "# Convert generators to lists\n",
        "json_data = {\n",
        "    \"en-indic\": list(en_to_te),\n",
        "    \"indic-en\": list(te_to_en)\n",
        "}\n",
        "\n",
        "# Save to a single JSON file\n",
        "with open(\"nllb_en_te.json\", \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(json_data, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "print(\"Dataset saved as nllb_en_te.json\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers datasets torch sentencepiece sacrebleu\n",
        "\n",
        "import torch\n",
        "from transformers import MarianMTModel, MarianTokenizer, Trainer, TrainingArguments\n",
        "from datasets import load_dataset\n",
        "\n",
        "# Load Pre-trained Model and Tokenizer\n",
        "model_name = \"ai4bharat/indictrans2-indic-en-dist-300m\"\n",
        "tokenizer = MarianTokenizer.from_pretrained(model_name)\n",
        "model = MarianMTModel.from_pretrained(model_name)\n",
        "\n",
        "# Load and Prepare Dataset\n",
        "dataset = load_dataset(\"json\", data_files=\"your_dataset.json\", split=\"train\")\n",
        "\n",
        "# Tokenization function\n",
        "def tokenize(batch):\n",
        "    src_texts = batch[\"src\"]\n",
        "    tgt_texts = batch[\"tgt\"]\n",
        "\n",
        "    src_encodings = tokenizer(src_texts, padding=\"max_length\", truncation=True, max_length=128, return_tensors=\"pt\")\n",
        "    tgt_encodings = tokenizer(tgt_texts, padding=\"max_length\", truncation=True, max_length=128, return_tensors=\"pt\")\n",
        "\n",
        "    return {\n",
        "        \"input_ids\": src_encodings[\"input_ids\"],\n",
        "        \"attention_mask\": src_encodings[\"attention_mask\"],\n",
        "        \"labels\": tgt_encodings[\"input_ids\"]\n",
        "    }\n",
        "\n",
        "# Tokenize dataset\n",
        "tokenized_dataset = dataset.map(tokenize, batched=True)\n",
        "\n",
        "# Training Arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./fine_tuned_model\",\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    learning_rate=3e-5,\n",
        "    per_device_train_batch_size=8,\n",
        "    per_device_eval_batch_size=8,\n",
        "    num_train_epochs=5,\n",
        "    weight_decay=0.01,\n",
        "    save_total_limit=2,\n",
        "    fp16=True if torch.cuda.is_available() else False,  # Enable mixed precision if GPU is available\n",
        "    logging_dir=\"./logs\",\n",
        ")\n",
        "\n",
        "# Trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_dataset,\n",
        "    tokenizer=tokenizer,\n",
        ")\n",
        "\n",
        "# Train the Model\n",
        "trainer.train()\n",
        "\n",
        "# Save Model and Tokenizer\n",
        "model.save_pretrained(\"fine_tuned_model\")\n",
        "tokenizer.save_pretrained(\"fine_tuned_model\")\n",
        "\n",
        "# Test Translation\n",
        "input_text = \"मुझे स्कूल जाना है।\"\n",
        "inputs = tokenizer(input_text, return_tensors=\"pt\", padding=True, truncation=True)\n",
        "output_ids = model.generate(**inputs)\n",
        "output_text = tokenizer.batch_decode(output_ids, skip_special_tokens=True)[0]\n",
        "\n",
        "print(\"Translated Output:\", output_text)  # Expected output: \"I have to go to school.\"\n"
      ],
      "metadata": {
        "id": "n3hgUNQAoWKg"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}